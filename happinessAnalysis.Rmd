---
title: "R Notebook"
output:
  html_notebook: default
  html_document: default
---
This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 
Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*. 
Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.
When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).
The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.
```{r}
library(corrplot)
library(dplyr)
library(ggplot2)
library(DescTools)
library(cluster)
# setwd("~/Downloads/DA/young-people-survey/youth-happiness-analysis/")
df <- read.csv("imputedResponses.csv",na.strings=c(""," ","NA"))
is.finite.data.frame <- function(obj){
    sapply(obj,FUN = function(x) all(is.finite(x)))
}
df <- df[rowSums(is.na(df)) == 0, ]
# df[rowSums(is.finite(df)) == 0, ]
# df <- na.omit(df[relevantAttributes1$Attributes])
```

```{r}
# As we studied in prior kernels,considering bmi instead if height and weight as separate metrics is better.
# We have established that the height is in cms and weight in kgs.

bmi = function(height,weight){
  return(weight/(height/100)^2)
}
df$bmi = bmi(df$Height,df$Weight)

hist(df$bmi,col = "blue",breaks = 100,xlim = c(12,60),main="Histogram For BMI")
# Since BMI is the numerical value of a scale different from other variables we redcode the data to the same scale.
# 1,2,3,4,5 being underweight,fit,healthy,overweight and obese respectively.
df$bmi[df$bmi <= 18.5] = 1
df$bmi[df$bmi > 18.5 & df$bmi <= 20] = 2
df$bmi[df$bmi > 20 & df$bmi <= 25] = 3
df$bmi[df$bmi > 25 & df$bmi <= 30] = 4
df$bmi[df$bmi > 30] = 5

nreqAttributes <- names(df) %in% c("Height","Weight")
df <- df[!nreqAttributes]
```

```{r}
happinessFactors <- c("Hypochondria","Loneliness","Dreams","Number.of.friends","Mood.swings",
                      "Getting.angry","Life.struggles","Happiness.in.life","Energy.levels","Personality")
happinessSadness <- df[happinessFactors]

# We consider the above mentoned variables as the factors of happiness and sadness.From their correlation 
# plot we can infer that none of the variables under study are very highly correlated.So,we use these ten 
# factors across sections of our dataset to find the variables which effect these factors the most and in 
# the end effect the happiness / sadness of people.
par(mfrow=c(2,5),mar=c(2,2,2,2))
for (factorV in happinessFactors){
  hist(happinessSadness[[factorV]],breaks = c(0,1,2,3,4,5),freq = FALSE,col="#3399FF",main="",mgp=c(1,0,0),xlab=factorV)
}

```

```{r}
modify <- function(x) x

modifiedHappinessSadness <- data.frame(happinessSadness[,c('Dreams','Number.of.friends','Happiness.in.life','Energy.levels','Personality')], lapply(happinessSadness[,c('Hypochondria','Loneliness','Mood.swings','Getting.angry','Life.struggles')], modify) )
pcaHappinessSadness <- prcomp(modifiedHappinessSadness)
pcaHappinessSadness = as.data.frame(pcaHappinessSadness$x[,1])
Happy <- vector(length = 986)
modifiedHappinessSadness = cbind(modifiedHappinessSadness,pcaHappinessSadness,Happy)
colnames(modifiedHappinessSadness)[11] <- "pcaHappinessSadness"
modifiedHappinessSadness$Happy[modifiedHappinessSadness$pcaHappinessSadness<0] = "FALSE"
modifiedHappinessSadness$Happy[modifiedHappinessSadness$pcaHappinessSadness>0] = "TRUE"
df$Happy <- modifiedHappinessSadness$Happy
happinessCount <- table(df$Happy,df$Gender)
barplot(happinessCount, main = paste("Happy vs Gender"), col = c("red","blue"))
legend("topright",legend = rownames(happinessCount),fill = c("red","blue") ,ncol = 1,cex = 0.4)
```

```{r}

findCorrelation <- function(workingData,predictorVariable){
  corrVals <- list()
i <- 1
for(variable in colnames(workingData)){
  # print(variable)
  corrVals[i] <- GoodmanKruskalGamma(workingData[[variable]],predictorVariable)
  i <- i + 1
}
corrVals <- as.numeric(corrVals)
setNames(corrVals,colnames(workingData))
corrVals <- data.frame(corrVals)

corrVals$Attributes = as.vector(colnames(workingData))
corrValsModified <- corrVals
corrValsModified$Attributes <- factor(corrValsModified$Attributes, levels = corrValsModified$Attributes[order(-corrValsModified$corrVals)])
relevantTraits <- corrValsModified[(corrValsModified$corrVals >= 0.25 | corrValsModified$corrVals <= -0.25),]
p <- ggplot(relevantTraits, aes(x=Attributes,y=corrVals,fill = Attributes)) + geom_bar(stat="identity") + scale_fill_hue() + coord_flip()+ggtitle("Relevant Attributes")
print(p)
print(relevantTraits$Attributes)
return(relevantTraits)
}

```

```{r}
# Analysis of various traits wrt Happy Label.
df$Happy[df$Happy == TRUE] = 1
df$Happy[df$Happy == FALSE] = 0
df$Happy = as.numeric(df$Happy)
workingData <- df
nreqVariables = names(workingData) %in% happinessFactors
# workingData$Happy = df$Happy

relevantAttributes1 <- findCorrelation(workingData = workingData[!nreqVariables],workingData$Happy)

```
```{r}
relevantAttributes2 <- findCorrelation(workingData = workingData[!(names(workingData) %in% c('Happy','Happiness.in.life'))],workingData$Happiness.in.life)
```

```{r}
library(clue)
library(factoextra)
library(caret)
library(scales)

```

```{r}
# Preparing Data for clustering,scaling all the attributes to 1-5,omitting categorical data.
rawData1 <- na.omit(df[as.vector(relevantAttributes1$Attributes)])
rawData1$Happy <- na.omit(df$Happy)
nreqAttributes <- names(rawData1) %in% c("bmi","Gender")
rawData1 <- rawData1[!nreqAttributes]
# Scale Age weight and height.
rawData1$Age <- rescale(rawData1$Age, to = c(1, 5), from = range(rawData1$Age))
nrowsTrain <- 0.8*nrow(rawData1)
trainData1 <- rawData1[1:nrowsTrain,]
actualData1 <- df[nrowsTrain:nrow(df),]
testData1 <- rawData1[nrowsTrain:nrow(rawData1),]
# resultsDF <- data.frame(matrix(NA,nrow = nrow(testData1)))
resultsDF <- data.frame(testData1$Happy)
```


```{r}

#K-Means clustering for the data
#Method 1
fviz_nbclust(trainData1, kmeans, method = "wss") +
  geom_vline(xintercept = 5, linetype = 2)
set.seed(123)
km.res <- kmeans(trainData1, 3, nstart = 25,iter.max = 10)
km.res$centers
fviz_cluster(km.res, data = trainData1)
```

```{r}
predictedCluster <- as.vector(cl_predict(km.res,testData1))
resultsDF$PredictedKmean = predictedCluster
resultsDF$PredictedKmean[resultsDF$PredictedKmean == 3| resultsDF$PredictedKmean == 2] = 0
resultsDF$PredictedKmean[resultsDF$PredictedKmean == 1] = 1
confusionMatrix(factor(actualData1$Happy,levels = 0:1),factor(resultsDF$PredictedKmean,levels = 0:1))
```

```{r}
rawData2 <- na.omit(df[as.vector(relevantAttributes2$Attributes)])
rawData2$Happiness.in.life <- na.omit(df$Happiness.in.life)
nrowsTrain <- 0.9*nrow(rawData2)
trainData2 <- rawData2[1:nrowsTrain,]
actualData <- df[nrowsTrain:nrow(df),]
testData2 <- rawData2[nrowsTrain:nrow(rawData2),]
```

```{r}
fviz_nbclust(trainData2, kmeans, method = "wss") +
  geom_vline(xintercept = 5, linetype = 2)
set.seed(123)
km.res <- kmeans(trainData2, 7, nstart = 25)
tempCenters <- as.data.frame(km.res$centers)
print(tempCenters)
fviz_cluster(km.res, data = trainData2)
```

```{r}
predictedCluster <- as.vector(cl_predict(km.res,testData2))
testData2$PredictedKmean2 = predictedCluster
testData2$PredictedKmean[testData2$PredictedKmean == 5] = 2
testData2$PredictedKmean[testData2$PredictedKmean == 3 ] = 3
testData2$PredictedKmean[testData2$PredictedKmean == 4 | testData2$PredictedKmean == 1| testData2$PredictedKmean == 7|testData2$PredictedKmean == 2 ] = 5
confusionMatrix(factor(actualData$Happiness.in.life,levels = 1:5),factor(testData2$PredictedKmean,levels = 1:5))


```

```{r}
# We obtain an accuracy of 74.51%
# 
# #Method 2
# set.seed(123)
# km_res2 <- eclust(rawData, "kmeans", k = 4,nstart = 25, graph = FALSE)
# fviz_cluster(km_res2, data=trainData)
#Note that, silhouette coefficient measures how well an observation is clustered and it estimates the average distance between clusters
#(i.e, the average silhouette width).Observations with negative silhouette are probably placed in the wrong cluster.
# fviz_silhouette(km_res2)
# predictedCluster <- as.vector(cl_predict(km.res2,testData))
# testData$Predicted = predictedCluster
# testData$Predicted[testData$Predicted <=2] = 0
# testData$Predicted[testData$Predicted >2] = 1
# print(table(as.factor(actualData$Happy),as.factor(testData$Predicted)))
# confusionMatrix(as.factor(actualData$Happy),as.factor(testData$Predicted))
#We get accuracy of 67.65% using this method

```

```{r}
#Hierarchical clustering for all the relevant attributes
##For k=2
library(dendextend)
library(colorspace)
distCalc <- dist(rawData1)
#Using the "complete" method for clustering as it gives better accuracy
hclusters <- hclust(distCalc, method = "complete")
happyLevels <- rev(levels(as.factor(rawData1$Happy)))

#Forming the dendrogram
dend <- as.dendrogram(hclusters)
dend <- rotate(dend, 1:150)
dend <- color_branches(dend, k=2)
labels_colors(dend) <-
  rainbow_hcl(2)[sort_levels_values(
    as.numeric(rawData1$Happy)[order.dendrogram(dend)]
  )]

labels(dend) <- paste(as.character(rawData1$Happy)[order.dendrogram(dend)],"(",labels(dend),")", sep = "")
dend <- hang.dendrogram(dend,hang_height=0.1)
dend <- set(dend, "labels_cex", 0.5)

#Plotting the dendrogram
par(mfrow=c(1,1))
plot(dend, 
     main = "Hierarchical clustering of relevant attributes", 
     horiz =  TRUE,  nodePar = list(cex = .007))
legend("topleft", legend = happyLevels, fill = rainbow_hcl(2))

hclusteringMethod <- c("complete")
hclustDendlist <- dendlist()
for(i in seq_along(hclusteringMethod)) {
  hCluster <- hclust(distCalc , method = hclusteringMethod[i])
  hclustDendlist <- dendlist(hclustDendlist, as.dendrogram(hCluster))
}
hclustDendlist

getClusters <- function(dend) {
  cutree(dend, k =2)[order.dendrogram(dend)]
}
dendClusters <- lapply(hclustDendlist, getClusters)
dendClusters<-as.data.frame(dendClusters)
colnames(dendClusters)[1]<-"predicted.clusters"
modifiedDendClusters<-dendClusters
for (i in 1:986)
{
  #If it is cluster 1,Happy value =0
  if(modifiedDendClusters$predicted.clusters[i]==1)
    modifiedDendClusters$predicted.clusters[i]<-0
  else
    #If it is cluster 2,Happy value =1
    modifiedDendClusters$predicted.clusters[i]<-1
}
#For the confusion Matrix
referenceData<-rawData1$Happy
predictedData<-modifiedDendClusters$predicted.clusters
unionData <- union(predictedData,referenceData)
tableData <- table(factor(predictedData, unionData), factor(referenceData, unionData))
confusionMatrix(tableData)
#We get an accuracy of 52.13%
```

```{r}
#Hierarchical clustering for testData1
library(dendextend)
library(colorspace)
distCalcTest <- dist(testData1)
#Using the "ward.D" method for clustering as it gives better accuracy
hclustersTest <- hclust(distCalcTest, method = "ward.D")
happyLevelsTest <- rev(levels(as.factor(testData1$Happy)))

#Forming the dendrogram
dendTest <- as.dendrogram(hclustersTest)
dendTest <- rotate(dendTest, 1:150)
dendTest <- color_branches(dendTest, k=2)
labels_colors(dendTest) <-
  rainbow_hcl(2)[sort_levels_values(
    as.numeric(testData1$Happy)[order.dendrogram(dendTest)]
  )]

labels(dend) <- paste(as.character(testData1$Happy)[order.dendrogram(dendTest)],"(",labels(dendTest),")", sep = "")
dendTest <- hang.dendrogram(dendTest,hang_height=0.1)
dendTest <- set(dendTest, "labels_cex", 0.5)

#Plotting the dendrogram
par(mfrow=c(1,1))
plot(dendTest, 
     main = "Hierarchical Clustering on Test Data", 
     horiz =  TRUE,  nodePar = list(cex = .007))
legend("topleft", legend = happyLevelsTest, fill = rainbow_hcl(2))

hclusteringMethod <- c("ward.D")
hclustDendlistTest <- dendlist()
for(i in seq_along(hclusteringMethod)) {
  hClusterTest <- hclust(distCalcTest , method = hclusteringMethod[i])
  hclustDendlistTest <- dendlist(hclustDendlistTest, as.dendrogram(hClusterTest))
}
#hclustDendlistTest

getClustersTest <- function(dendTest) {
  cutree(dendTest, k =2)[order.dendrogram(dendTest)]
}
dendClustersTest <- lapply(hclustDendlistTest, getClustersTest)
dendClustersTest<-as.data.frame(dendClustersTest)
colnames(dendClustersTest)[1]<-"predictedHierarchical"
modifiedDendClustersTest<-dendClustersTest
for (i in 1:nrow(modifiedDendClustersTest))
{
  #If it is cluster 1,Happy value =0
  if(modifiedDendClustersTest$predictedHierarchical[i]==1)
    modifiedDendClustersTest$predictedHierarchical[i]<-0
  else
    #If it is cluster 2,Happy value =1
    modifiedDendClustersTest$predictedHierarchical[i]<-1
}

#Check
resultsDF$predictedHierarchical<-modifiedDendClustersTest

#For the confusion Matrix
referenceDataTest<-testData1$Happy
predictedDataHierarchicalTest<-modifiedDendClustersTest$predictedHierarchical
unionDataHierarchicalTest <- union(predictedDataHierarchicalTest,referenceDataTest)
tableDataHierarchicalTest <- table(factor(predictedDataHierarchicalTest, unionDataHierarchicalTest), factor(referenceDataTest, unionDataHierarchicalTest))
confusionMatrix(tableDataHierarchicalTest)
#We get an accuracy of 63.13%
```


```{r}
#Decision Tree using CART method
set.seed(1)
library(rpart)
library(rpart.plot)

fit <- rpart(Happy~., data = trainData1, method = 'class')
rpart.plot(fit, extra = 101,main="Decision Tree using CART method")

predictedVal <-predict(fit, testData1, type = 'class')

tableMat <- table(testData1$Happy, predictedVal)
confusionMatrix(tableMat,positive = '1',mode="prec_recall")


```
C5.0 is an extension of ID3 algorithm to generate a decision tree and uses normalized information gain as the criteria to split the samples into a class.

```{r}
#Decision Tree using C5.0
library(C50)
set.seed(123)
model <- C5.0(factor(Happy)~Health+Changing.the.past+Interests.or.hobbies+Fear.of.public.speaking+Romantic,data=trainData1)
plot(model,main="Decision Tree for C5.0 method")
resultsDF$predictedDTC5 = predict(model,testData1)
confusionMatrix(factor(resultsDF$predictedDTC5),factor(testData1$Happy),positive = '1',mode="prec_recall")

```


Random Forest is an ensemble classification method that generates multiple decision trees and outputs the mode of the classes.
```{r}
#Decision tree using Random Forest


library(randomForest)
set.seed(1)
rf = randomForest(factor(Happy)~.,data = trainData1,ntree=750)
plot(rf)
varImpPlot(rf,main="Importance of variables")
resultsDF$predictedRf = predict(rf,testData1)
confusionMatrix(factor(resultsDF$predictedRf),factor(testData1$Happy),positive = '1',mode="prec_recall")

```


```{r}
#SVM linear

svmLinearModel = svm(Happy~.,data = trainData1,type="C-classification",kernel="linear")
resultsDF$predictedSvmLinear = predict(svmLinearModel,testData1)
confusionMatrix(factor(resultsDF$predictedSvmLinear),factor(testData1$Happy),positive = '1',mode="prec_recall")
plot(svmLinearModel,data=trainData1,Changing.the.past ~ Interests.or.hobbies) 
```
```{r}
#SVM polynomial

svmPolyModel = svm(Happy~.,data = trainData1,type="C-classification",kernel="polynomial")
resultsDF$predictedSvmPoly = predict(svmPolyModel,testData1)
confusionMatrix(factor(resultsDF$predictedSvmPoly),factor(testData1$Happy),positive = '1',mode="prec_recall")
plot(svmPolyModel,data=trainData1,Changing.the.past ~ Interests.or.hobbies) 

```

```{r}
#SVM radial 

svmRadialModel = svm(Happy~.,data = trainData1,type="C-classification",kernel="radial")
resultsDF$predictedSvmRadial = predict(svmRadialModel,testData1)
confusionMatrix(factor(resultsDF$predictedSvmRadial),factor(testData1$Happy),positive = '1',mode="prec_recall")
plot(svmPolyModel,data=trainData1,Changing.the.past ~ Interests.or.hobbies) 

```

```{r}

#Pam Clustering

library(cluster)

set.seed(123)
pam.res <- pam(trainData1, 6)
pam.res$medoids
fviz_cluster(pam.res, data = trainData1)
```

```{r}
predictedClusterPam <- as.vector(cl_predict(pam.res,testData1))
resultsDF$PredictedPam = predictedClusterPam
resultsDF$PredictedPam[resultsDF$PredictedPam == 3 |resultsDF$PredictedPam == 5 | resultsDF$PredictedPam == 6] = 0
resultsDF$PredictedPam[resultsDF$PredictedPam == 1| resultsDF$PredictedPam == 2 | resultsDF$PredictedPam == 4] = 1
confusionMatrixPam = confusionMatrix(factor(resultsDF$PredictedPam,levels = 0:1),factor(testData1$Happy,levels = 0:1),positive = '1',mode="prec_recall")
print(confusionMatrixPam)

```

```{r}
library(flexclust)
# K Median Clustering
set.seed(12)
#To iterate and train the model 15 times
#Considering k=4
for(i in 1:15)
{
  kMedian = kcca(trainData1, k=4, kccaFamily("kmedians"),save.data = TRUE)
}
kMedianValues = parameters(kMedian)
#To print the median value of each cluster
kMedianValues
kMedianTrainClusters<-clusters(kMedian)
clusplot(trainData1,kMedianTrainClusters,main = paste("CLUSPLOT For K Medians(k=4)"))
predClusterMedian<- predict(kMedian, newdata=testData1, k=4, kccaFamily("kmedians"))
```


```{r}
resultsDF$predictedKMedian = predClusterMedian
resultsDF$predictedKMedian[resultsDF$predictedKMedian == 2| resultsDF$predictedKMedian  == 4] = 0
resultsDF$predictedKMedian[resultsDF$predictedKMedian == 1| resultsDF$predictedKMedian  == 3] = 1
confusionMatrix(factor(testData1$Happy,levels = 0:1),factor(resultsDF$predictedKMedian ,levels = 0:1))
```


```{r}
# set.seed(1)
library(e1071)
cmeansCluster <- cmeans(trainData1,centers = 3 ,iter.max = 100, verbose = FALSE,dist = "manhattan", method = "cmeans", m = 2,rate.par = NULL, weights = 1, control = list())
cmeansCluster$centers
clusplot(trainData1,cmeansCluster$cluster)
```

```{r}
resultsDF$PredictedCmeans <- cl_predict(cmeansCluster,testData1,type = "class_ids")
resultsDF$PredictedCmeans[resultsDF$PredictedCmeans == 3] = 0
resultsDF$PredictedCmeans[resultsDF$PredictedCmeans == 1| resultsDF$PredictedCmeans == 2] = 1
confusionMatrix(factor(actualData1$Happy,levels = 0:1),factor(resultsDF$PredictedCmeans,levels = 0:1))
```
Majority vote: It’s defined as taking the prediction with maximum vote / recommendation from multiple models predictions while predicting the outcomes of a classification problem.
```{r}
determineMajority <- function(){
  for(i in 1:nrow(resultsDF)) 
  {
    if(rowSums(resultsDF[i,][2:5]) >= 2){
      resultsDF$PredictedVote[i] = 1
    }
    else{
      resultsDF$PredictedVote[i] = 0
    }
  }
  return (resultsDF$PredictedVote)
}

resultsDF$PredictedVote = 1:nrow(resultsDF)
resultsDF$PredictedVote = determineMajority()
confusionMatrix(factor(actualData1$Happy,levels = 0:1),factor(resultsDF$PredictedVote,levels = 0:1))
```


Weighted average: In this, different weights are applied to predictions from multiple models then taking the average which means giving high or low importance to specific model output.
Assigning weights based on the accuracies of the models.
We address the task of choosing prior weights for models that are to be used for weighted model averaging.
Models that are very similar should usually be given smaller weights than models that are quite distinct.
The weighting schemes give smaller weights to models that are more highly correlated. Other desirable properties of a weighting scheme are identified, and we examine the extent to which these properties are held by the proposed methods.
The weighting schemes are applied to real data, and prior weights, posterior weights and Bayesian model averages are determined. For these data, empirical Bayes methods were used to form the correlation matrices that yield the prior weights. Predictive variances are examined, as empirical Bayes methods can result in unrealistically small variances.
```{r}
corrMatrix <- cor(resultsDF)
corrplot(corrMatrix,method = "number",title = "Correlation Between Different Models",mar = c(0,0,2,0))
corrplot(cov(resultsDF),method = "number",title = "Covariace Between Different Models",mar = c(0,0,2,0))
```
From the above results we observe there is no significant correlation or covariance between two models.Hence considering all of them is essential.The weights can hence be assigned according to the accuracies of the different models.
Cmeans - 0.33
Kmeans - 0.25
KMedoids - 0.24
KMedian - 0.28
```{r}
# table(resultsDF)
wAverageRow <- function(row){
  weights <- c(2,3,3,7)/15
  return(weighted.mean(row,weights))
}
wAverageDetermination <- function(){
  for(i in 1:nrow(resultsDF)) 
  {
    resultsDF$PredictedWA[i] = wAverageRow(resultsDF[i,][2:5])
    if(resultsDF$PredictedWA[i] >= 0.5){
      resultsDF$PredictedVote[i] = 1
    }
    else{
      resultsDF$PredictedVote[i] = 0
    }
  }
  return (resultsDF$PredictedWA)
}
resultsDF$PredictedWA = 1:nrow(resultsDF)
resultsDF$PredictedWA = wAverageDetermination()
confusionMatrix(factor(actualData1$Happy,levels = 0:1),factor(resultsDF$PredictedWA,levels = 0:1))
# hist(resultsDF$PredictedWA)
```

