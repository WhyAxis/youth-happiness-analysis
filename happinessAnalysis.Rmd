---
title: "R Notebook"
output:
  html_document: default
  html_notebook: default
---
This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 
Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*. 
Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.
When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).
The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.
```{r}
library(corrplot)
library(dplyr)
library(ggplot2)
library(DescTools)
library(cluster)
# setwd("~/Downloads/DA/young-people-survey/youth-happiness-analysis/")
df <- read.csv("imputedResponses.csv",na.strings=c(""," ","NA"))
is.finite.data.frame <- function(obj){
    sapply(obj,FUN = function(x) all(is.finite(x)))
}
df <- df[rowSums(is.na(df)) == 0, ]
# df[rowSums(is.finite(df)) == 0, ]
# df <- na.omit(df[relevantAttributes1$Attributes])
```

```{r}
# As we studied in prior kernels,considering bmi instead if height and weight as separate metrics is better.
# We have established that the height is in cms and weight in kgs.

bmi = function(height,weight){
  return(weight/(height/100)^2)
}
df$bmi = bmi(df$Height,df$Weight)

hist(df$bmi,col = "blue",breaks = 100,xlim = c(12,60))
# Since BMI is the numerical value of a scale different from other variables we redcode the data to the same scale.
# 1,2,3,4,5 being underweight,fit,healthy,overweight and obese respectively.
df$bmi[df$bmi <= 18.5] = 1
df$bmi[df$bmi > 18.5 & df$bmi <= 20] = 2
df$bmi[df$bmi > 20 & df$bmi <= 25] = 3
df$bmi[df$bmi > 25 & df$bmi <= 30] = 4
df$bmi[df$bmi > 30] = 5

nreqAttributes <- names(df) %in% c("Height","Weight")
df <- df[!nreqAttributes]
```

```{r}
happinessFactors <- c("Hypochondria","Loneliness","Dreams","Number.of.friends","Mood.swings",
                      "Getting.angry","Life.struggles","Happiness.in.life","Energy.levels","Personality")
happinessSadness <- df[happinessFactors]

# We consider the above mentoned variables as the factors of happiness and sadness.From their correlation 
# plot we can infer that none of the variables under study are very highly correlated.So,we use these ten 
# factors across sections of our dataset to find the variables which effect these factors the most and in 
# the end effect the happiness / sadness of people.
par(mfrow=c(2,5),mar=c(2,2,2,2))
for (factorV in happinessFactors){
  hist(happinessSadness[[factorV]],breaks = c(0,1,2,3,4,5),freq = FALSE,col="#3399FF",main="",mgp=c(1,0,0),xlab=factorV)
}

```

```{r}
modify <- function(x) 5-x

modifiedHappinessSadness <- data.frame(happinessSadness[,c('Dreams','Number.of.friends','Happiness.in.life','Energy.levels','Personality')], lapply(happinessSadness[,c('Hypochondria','Loneliness','Mood.swings','Getting.angry','Life.struggles')], modify) )
pcaHappinessSadness <- prcomp(modifiedHappinessSadness)
pcaHappinessSadness = as.data.frame(pcaHappinessSadness$x[,1])
Happy <- vector(length = 986)
modifiedHappinessSadness = cbind(modifiedHappinessSadness,pcaHappinessSadness,Happy)
colnames(modifiedHappinessSadness)[11] <- "pcaHappinessSadness"
modifiedHappinessSadness$Happy[modifiedHappinessSadness$pcaHappinessSadness<0] = "FALSE"
modifiedHappinessSadness$Happy[modifiedHappinessSadness$pcaHappinessSadness>0] = "TRUE"
df$Happy <- modifiedHappinessSadness$Happy
happinessCount <- table(df$Happy,df$Gender)
barplot(happinessCount, main = paste("Happy vs Gender"), col = c("red","blue"))
legend("topright",legend = rownames(happinessCount),fill = c("red","blue") ,ncol = 1,cex = 0.4)
```

```{r}

findCorrelation <- function(workingData,predictorVariable){
  corrVals <- list()
i <- 1
for(variable in colnames(workingData)){
  # print(variable)
  corrVals[i] <- GoodmanKruskalGamma(workingData[[variable]],predictorVariable)
  i <- i + 1
}
corrVals <- as.numeric(corrVals)
setNames(corrVals,colnames(workingData))
corrVals <- data.frame(corrVals)

corrVals$Attributes = as.vector(colnames(workingData))
corrValsModified <- corrVals
corrValsModified$Attributes <- factor(corrValsModified$Attributes, levels = corrValsModified$Attributes[order(-corrValsModified$corrVals)])
relevantTraits <- corrValsModified[(corrValsModified$corrVals >= 0.25 | corrValsModified$corrVals <= -0.25),]
p <- ggplot(relevantTraits, aes(x=Attributes,y=corrVals,fill = Attributes)) + geom_bar(stat="identity") + scale_fill_hue() + coord_flip()
print(p)
print(relevantTraits$Attributes)
return(relevantTraits)
}

```

```{r}
# Analysis of various traits wrt Happy Label.
df$Happy[df$Happy == TRUE] = 1
df$Happy[df$Happy == FALSE] = 0
df$Happy = as.numeric(df$Happy)
workingData <- df
nreqVariables = names(workingData) %in% happinessFactors
# workingData$Happy = df$Happy

relevantAttributes1 <- findCorrelation(workingData = workingData[!nreqVariables],workingData$Happy)

```
```{r}
relevantAttributes2 <- findCorrelation(workingData = workingData[!(names(workingData) %in% c('Happy','Happiness.in.life'))],workingData$Happiness.in.life)
```

```{r}
library(clue)
library(factoextra)
library(caret)
library(scales)

```

```{r}
# Preparing Data for clustering,scaling all the attributes to 1-5,omitting categorical data.
rawData1 <- na.omit(df[as.vector(relevantAttributes1$Attributes)])
rawData1$Happy <- na.omit(df$Happy)
nreqAttributes <- names(rawData1) %in% c("bmi","Gender")
rawData1 <- rawData1[!nreqAttributes]
# Scale Age weight and height.
rawData1$Age <- rescale(rawData1$Age, to = c(1, 5), from = range(rawData1$Age))
nrowsTrain <- 0.8*nrow(rawData1)
trainData1 <- rawData1[1:nrowsTrain,]
actualData1 <- df[nrowsTrain:nrow(df),]
testData1 <- rawData1[nrowsTrain:nrow(rawData1),]
resultsDF <- testData1$Happy
```


```{r}

#K-Means clustering for the data
#Method 1
fviz_nbclust(trainData1, kmeans, method = "wss") +
  geom_vline(xintercept = 5, linetype = 2)
set.seed(123)
km.res <- kmeans(trainData1, 3, nstart = 25,iter.max = 10)
km.res$centers
fviz_cluster(km.res, data = trainData1)
```

```{r}
predictedCluster <- as.vector(cl_predict(km.res,testData1))
resultsDF$PredictedKmean = predictedCluster
resultsDF$PredictedKmean[resultsDF$PredictedKmean == 3| resultsDF$PredictedKmean == 2] = 0
resultsDF$PredictedKmean[resultsDF$PredictedKmean == 1] = 1
confusionMatrix(factor(actualData1$Happy,levels = 0:1),factor(resultsDF$PredictedKmean,levels = 0:1))
```

```{r}
rawData2 <- na.omit(df[as.vector(relevantAttributes2$Attributes)])
rawData2$Happiness.in.life <- na.omit(df$Happiness.in.life)
nrowsTrain <- 0.9*nrow(rawData2)
trainData2 <- rawData2[1:nrowsTrain,]
actualData <- df[nrowsTrain:nrow(df),]
testData2 <- rawData2[nrowsTrain:nrow(rawData2),]
```

```{r}
fviz_nbclust(trainData2, kmeans, method = "wss") +
  geom_vline(xintercept = 5, linetype = 2)
set.seed(123)
km.res <- kmeans(trainData2, 4, nstart = 25)
tempCenters <- as.data.frame(km.res$centers)
print(tempCenters)
fviz_cluster(km.res, data = trainData2)
```

```{r}
predictedCluster <- as.vector(cl_predict(km.res,testData2))
testData2$PredictedKmean2 = predictedCluster

# testData2$PredictedKmean[testData2$PredictedKmean == 5] = 2
# testData2$PredictedKmean[testData2$PredictedKmean == 3 ] = 3
# testData2$PredictedKmean[testData2$PredictedKmean == 4 | testData2$PredictedKmean == 1| testData2$PredictedKmean == 7|testData2$PredictedKmean == 2 ] = 5
# confusionMatrix(factor(actualData$Happiness.in.life,levels = 1:5),factor(testData2$PredictedKmean,levels = 1:5))

testData2$PredictedKmean2[testData2$PredictedKmean2 == 1] = tempCenters$Happiness.in.life[1]
testData2$PredictedKmean2[testData2$PredictedKmean2 == 2] = tempCenters$Happiness.in.life[2]
testData2$PredictedKmean2[testData2$PredictedKmean2 == 3] = tempCenters$Happiness.in.life[3]
testData2$PredictedKmean2[testData2$PredictedKmean2 == 4] = tempCenters$Happiness.in.life[4]
plot(testData2$PredictedKmean2,testData2$Happiness.in.life)
# plot(density(resid(testData2$Happiness.in.life~testData2$PredictedKmean2)))
model1 <- lm(testData2$Happiness.in.life~testData2$PredictedKmean2)
plot(density(resid(model1)))
# qqnorm(resid(model1))
# qqline(resid(model1))
# resid(testData2$Happiness.in.life~testData2$PredictedKmean2)

```

```{r}
# We obtain an accuracy of 74.51%
# 
# #Method 2
# set.seed(123)
# km_res2 <- eclust(rawData, "kmeans", k = 4,nstart = 25, graph = FALSE)
# fviz_cluster(km_res2, data=trainData)
#Note that, silhouette coefficient measures how well an observation is clustered and it estimates the average distance between clusters
#(i.e, the average silhouette width).Observations with negative silhouette are probably placed in the wrong cluster.
# fviz_silhouette(km_res2)
# predictedCluster <- as.vector(cl_predict(km.res2,testData))
# testData$Predicted = predictedCluster
# testData$Predicted[testData$Predicted <=2] = 0
# testData$Predicted[testData$Predicted >2] = 1
# print(table(as.factor(actualData$Happy),as.factor(testData$Predicted)))
# confusionMatrix(as.factor(actualData$Happy),as.factor(testData$Predicted))
#We get accuracy of 67.65% using this method

```

```{r}
#Hierarchical clustering
##For k=2
library(dendextend)
library(colorspace)
distCalc <- dist(rawData1)
#Using the "complete" method for clustering as it gives better accuracy
hclusters <- hclust(distCalc, method = "complete")
happyLevels <- rev(levels(as.factor(rawData1$Happy)))

#Forming the dendrogram
dend <- as.dendrogram(hclusters)
dend <- rotate(dend, 1:150)
dend <- color_branches(dend, k=2)
labels_colors(dend) <-
  rainbow_hcl(2)[sort_levels_values(
    as.numeric(rawData1$Happy)[order.dendrogram(dend)]
  )]

labels(dend) <- paste(as.character(rawData1$Happy)[order.dendrogram(dend)],"(",labels(dend),")", sep = "")
dend <- hang.dendrogram(dend,hang_height=0.1)
dend <- set(dend, "labels_cex", 0.5)

#Plotting the dendrogram
par(mfrow=c(1,1))
plot(dend, 
     main = "Clustered data set", 
     horiz =  TRUE,  nodePar = list(cex = .007))
legend("topleft", legend = happyLevels, fill = rainbow_hcl(2))

hclusteringMethod <- c("complete")
hclustDendlist <- dendlist()
for(i in seq_along(hclusteringMethod)) {
  hCluster <- hclust(distCalc , method = hclusteringMethod[i])
  hclustDendlist <- dendlist(hclustDendlist, as.dendrogram(hCluster))
}
hclustDendlist

getClusters <- function(dend) {
  cutree(dend, k =2)[order.dendrogram(dend)]
}
dendClusters <- lapply(hclustDendlist, getClusters)
dendClusters<-as.data.frame(dendClusters)
colnames(dendClusters)[1]<-"predicted.clusters"
modifiedDendClusters<-dendClusters
for (i in 1:986)
{
  #If it is cluster 1,Happy value =0
  if(modifiedDendClusters$predicted.clusters[i]==1)
    modifiedDendClusters$predicted.clusters[i]<-0
  else
    #If it is cluster 2,Happy value =1
    modifiedDendClusters$predicted.clusters[i]<-1
}
#For the confusion Matrix
referenceData<-rawData1$Happy
predictedData<-modifiedDendClusters$predicted.clusters
unionData <- union(predictedData,referenceData)
tableData <- table(factor(predictedData, unionData), factor(referenceData, unionData))
confusionMatrix(tableData)

```

```{r}
#Decision Tree 

library(rpart)
library(rpart.plot)

fit <- rpart(Happy~Health+Changing.the.past+Public.speaking+Interests.or.hobbies+Storm+Darkness+Spiders+Fear.of.public.speaking+Cars+Active.sport+Adrenaline.sports+Romantic+Western+Action, data = trainData1, method = 'class')
rpart.plot(fit, extra = 101)

predictedVal <-predict(fit, testData1, type = 'class')

tableMat <- table(testData1$Happy, predictedVal)
confusionMatrix(tableMat)


```


```{r}

#Pam Clustering

library(cluster)

set.seed(123)
pam.res <- pam(trainData1, 6)
pam.res$medoids
fviz_cluster(pam.res, data = trainData1)
```

```{r}
predictedClusterPam <- as.vector(cl_predict(pam.res,testData1))
resultsDF$PredictedPam = predictedClusterPam
resultsDF$PredictedPam[resultsDF$PredictedPam == 3 |resultsDF$PredictedPam == 5 | resultsDF$PredictedPam == 6] = 0
resultsDF$PredictedPam[resultsDF$PredictedPam == 1| resultsDF$PredictedPam == 2 | resultsDF$PredictedPam == 4] = 1
confusionMatrixPam = confusionMatrix(factor(resultsDF$PredictedPam,levels = 0:1),factor(testData1$Happy,levels = 0:1))
print(confusionMatrixPam)

#Considering Positive Class as '1'
recallPam = confusionMatrixPam$table[4] / (confusionMatrixPam$table[4]+confusionMatrixPam$table[2])
precisionPam = confusionMatrixPam$table[4] /(confusionMatrixPam$table[4]+confusionMatrixPam$table[3])
fScorePam = (2*recallPam*precisionPam)/(recallPam+precisionPam)
fScorePam

```

```{r}
library(flexclust)
#K Median Clustering
set.seed(12)
#To iterate and train the model 15 times
#Considering k=4
for(i in 1:15)
{
  kMedian = kcca(trainData1, k=4, kccaFamily("kmedians"),save.data = TRUE)
}

#kMedian
kMedianValues = parameters(kMedian)
#To print the median value of each cluster
kMedianValues
kMedianTrainClusters<-clusters(kMedian)
clusplot(trainData1,kMedianTrainClusters,main = paste("CLUSPLOT For K Medians(k=4)"))

predClusterMedian<- predict(kMedian, newdata=testData1, k=4, kccaFamily("kmedians"))
#predClusterMedian
```


```{r}
resultsDF$predictedKMedian = predClusterMedian
resultsDF$predictedKMedian[resultsDF$predictedKMedian == 2| resultsDF$predictedKMedian  == 4] = 0
resultsDF$predictedKMedian[resultsDF$predictedKMedian == 1| resultsDF$predictedKMedian  == 3] = 1

confusionMatrix(factor(testData1$Happy,levels = 0:1),factor(resultsDF$predictedKMedian ,levels = 0:1))
```


```{r}
library(e1071)
cmeansCluster <- cmeans(trainData1,centers = 3 ,iter.max = 100, verbose = FALSE,dist = "manhattan", method = "cmeans", m = 2,rate.par = NULL, weights = 1, control = list())
cmeansCluster$centers
clusplot(trainData1,cmeansCluster$cluster)
```

```{r}
resultsDF$PredictedCmeans <- cl_predict(cmeansCluster,testData1,type = "class_ids")
resultsDF$PredictedCmeans[resultsDF$PredictedCmeans == 3] = 0
resultsDF$PredictedCmeans[resultsDF$PredictedCmeans == 1| resultsDF$PredictedCmeans == 2] = 1
confusionMatrix(factor(actualData1$Happy,levels = 0:1),factor(resultsDF$PredictedCmeans,levels = 0:1))
```

