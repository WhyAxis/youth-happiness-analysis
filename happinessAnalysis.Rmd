---
title: "R Notebook"
output:
  html_document: default
  html_notebook: default
---
This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 
Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*. 
Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.
When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).
The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.
```{r}
library(corrplot)
library(dplyr)
library(ggplot2)
library(DescTools)
library(cluster)
setwd("D:/5th sem/Projects/DA/Git/youth-happiness-analysis")
df <- read.csv("imputedResponses.csv",na.strings=c(""," ","NA"))
is.finite.data.frame <- function(obj){
    sapply(obj,FUN = function(x) all(is.finite(x)))
}
df <- df[rowSums(is.na(df)) == 0, ]
# df[rowSums(is.finite(df)) == 0, ]
# df <- na.omit(df[relevantAttributes1$Attributes])
```

```{r}
# As we studied in prior kernels,considering bmi instead if height and weight as separate metrics is better.
# We have established that the height is in cms and weight in kgs.

bmi = function(height,weight){
  return(weight/(height/100)^2)
}
df$bmi = bmi(df$Height,df$Weight)

hist(df$bmi,col = "blue",breaks = 100,xlim = c(12,60))
# Since BMI is the numerical value of a scale different from other variables we redcode the data to the same scale.
# 1,2,3,4,5 being underweight,fit,healthy,overweight and obese respectively.
df$bmi[df$bmi <= 18.5] = 1
df$bmi[df$bmi > 18.5 & df$bmi <= 20] = 2
df$bmi[df$bmi > 20 & df$bmi <= 25] = 3
df$bmi[df$bmi > 25 & df$bmi <= 30] = 4
df$bmi[df$bmi > 30] = 5
```

```{r}
happinessFactors <- c("Hypochondria","Loneliness","Dreams","Number.of.friends","Mood.swings",
                      "Getting.angry","Life.struggles","Happiness.in.life","Energy.levels","Personality")
happinessSadness <- df[happinessFactors]

# We consider the above mentoned variables as the factors of happiness and sadness.From their correlation 
# plot we can infer that none of the variables under study are very highly correlated.So,we use these ten 
# factors across sections of our dataset to find the variables which effect these factors the most and in 
# the end effect the happiness / sadness of people.
par(mfrow=c(2,5),mar=c(2,2,2,2))
for (factorV in happinessFactors){
  hist(happinessSadness[[factorV]],breaks = c(0,1,2,3,4,5),freq = FALSE,col="#3399FF",main="",mgp=c(1,0,0),xlab=factorV)
}

```

```{r}
modify <- function(x) 5-x

modifiedHappinessSadness <- data.frame(happinessSadness[,c('Dreams','Number.of.friends','Happiness.in.life','Energy.levels','Personality')], lapply(happinessSadness[,c('Hypochondria','Loneliness','Mood.swings','Getting.angry','Life.struggles')], modify) )
pcaHappinessSadness <- prcomp(modifiedHappinessSadness)
pcaHappinessSadness = as.data.frame(pcaHappinessSadness$x[,1])
Happy <- vector(length = 986)
modifiedHappinessSadness = cbind(modifiedHappinessSadness,pcaHappinessSadness,Happy)
colnames(modifiedHappinessSadness)[11] <- "pcaHappinessSadness"
modifiedHappinessSadness$Happy[modifiedHappinessSadness$pcaHappinessSadness<0] = "FALSE"
modifiedHappinessSadness$Happy[modifiedHappinessSadness$pcaHappinessSadness>0] = "TRUE"
df$Happy <- modifiedHappinessSadness$Happy
happinessCount <- table(df$Happy,df$Gender)
barplot(happinessCount, main = paste("Happy vs Gender"), col = c("red","blue"))
legend("topright",legend = rownames(happinessCount),fill = c("red","blue") ,ncol = 1,cex = 0.6)
```
```{r}

findCorrelation <- function(workingData,predictorVariable){
  corrVals <- list()
i <- 1
for(variable in colnames(workingData)){
  # print(variable)
  corrVals[i] <- GoodmanKruskalGamma(workingData[[variable]],predictorVariable)
  i <- i + 1
}
corrVals <- as.numeric(corrVals)
setNames(corrVals,colnames(workingData))
corrVals <- data.frame(corrVals)

corrVals$Attributes = as.vector(colnames(workingData))
corrValsModified <- corrVals
corrValsModified$Attributes <- factor(corrValsModified$Attributes, levels = corrValsModified$Attributes[order(-corrValsModified$corrVals)])
relevantTraits <- corrValsModified[(corrValsModified$corrVals >= 0.25 | corrValsModified$corrVals <= -0.25),]
p <- ggplot(relevantTraits, aes(x=Attributes,y=corrVals,fill = Attributes)) + geom_bar(stat="identity") + scale_fill_hue() + coord_flip()
print(p)
print(relevantTraits$Attributes)
return(relevantTraits)
}

```

```{r}
# Analysis of various traits wrt Happy Label.
df$Happy[df$Happy == TRUE] = 1
df$Happy[df$Happy == FALSE] = 0
df$Happy = as.numeric(df$Happy)
workingData <- df
nreqVariables = names(workingData) %in% happinessFactors
# workingData$Happy = df$Happy

relevantAttributes1 <- findCorrelation(workingData = workingData[!nreqVariables],workingData$Happy)

```
```{r}
relevantAttributes2 <- findCorrelation(workingData = workingData[!(names(workingData) %in% c('Happy','Happiness.in.life'))],workingData$Happiness.in.life)
```

```{r}
# clusterVar <- workingData[relevantAttributes1$Attributes]
# clusterVar$Happy <-  as.numeric(workingData$Happy)
# clusterVar$Happiness.in.life <- as.numeric(workingData$Happiness.in.life)
# noOfRowsTrainData <- 0.9*nrow(clusterVar)
# trainData <- clusterVar[1:noOfRowsTrainData,]
# testData <- clusterVar[noOfRowsTrainData:nrow(clusterVar),]

```


```{r}
library(clue)
library(factoextra)
library(caret)
library(scales)
```

```{r}
# Preparing Data for clustering,scaling all the attributes to 1-5,omitting categorical data.
rawData <- na.omit(df[as.vector(relevantAttributes1$Attributes)])
rawData$Happy <- na.omit(df$Happy)
nreqAttributes <- names(rawData) %in% c("Height","Weight","House...block.of.flats","Education","Left...right.handed","Village...town","Gender","Only.child","X",'Number.of.siblings')
rawData <- rawData[!nreqAttributes]
# Scale Age weight and height.
rawData$Age <- rescale(rawData$Age, to = c(1, 5), from = range(rawData$Age))
# rawData$Height <- rescale(rawData$Height, to = c(1, 5), from = range(rawData$Height))
# rawData$Weight <- rescale(rawData$Weight, to = c(1, 5), from = range(rawData$Weight))
nrowsTrain <- 0.9*nrow(rawData)
trainData <- rawData[1:nrowsTrain,]
actualData <- df[nrowsTrain:nrow(df),]
testData <- rawData[nrowsTrain:nrow(rawData),]
```


```{r}

#K-Means clustering for the data
#Method 1
fviz_nbclust(trainData, kmeans, method = "wss") +
  geom_vline(xintercept = 5, linetype = 2)
set.seed(123)
km.res <- kmeans(trainData, 3, nstart = 25)
km.res$centers
fviz_cluster(km.res, data = trainData)
```

```{r}
predictedCluster <- as.vector(cl_predict(km.res,testData))

testData$Predicted = predictedCluster
testData$Predicted[testData$Predicted == 4| testData$Predicted == 1| testData$Predicted == 2] = 0
testData$Predicted[testData$Predicted == 5| testData$Predicted == 3] = 1
confusionMatrix(factor(actualData$Happy,levels = 0:1),factor(testData$Predicted,levels = 0:1))
```

```{r}
rawData2 <- na.omit(df[as.vector(relevantAttributes2$Attributes)])
rawData2$Happiness.in.life <- na.omit(df$Happiness.in.life)
nrowsTrain <- 0.9*nrow(rawData2)
trainData2 <- rawData2[1:nrowsTrain,]
actualData <- df[nrowsTrain:nrow(df),]
testData2 <- rawData2[nrowsTrain:nrow(rawData),]
```

```{r}
fviz_nbclust(trainData2, kmeans, method = "wss") +
  geom_vline(xintercept = 5, linetype = 2)
set.seed(123)
km.res <- kmeans(trainData2, 8, nstart = 25)
km.res$centers
fviz_cluster(km.res, data = trainData2)
```

```{r}
predictedCluster <- as.vector(cl_predict(km.res,testData2))

testData2$Predicted = predictedCluster
testData2$Predicted[testData2$Predicted == 6] = 1
testData2$Predicted[testData2$Predicted == 5] = 2
testData2$Predicted[testData2$Predicted == 3 ] = 3
testData2$Predicted[testData2$Predicted == 4 | testData2$Predicted == 1| testData2$Predicted == 7|testData2$Predicted == 2 ] = 5
confusionMatrix(factor(actualData$Happiness.in.life,levels = 1:5),factor(testData2$Predicted,levels = 1:5))

```

```{r}
# We obtain an accuracy of 74.51%
# 
# #Method 2
# set.seed(123)
# km_res2 <- eclust(rawData, "kmeans", k = 4,nstart = 25, graph = FALSE)
# fviz_cluster(km_res2, data=trainData)
#Note that, silhouette coefficient measures how well an observation is clustered and it estimates the average distance between clusters
#(i.e, the average silhouette width).Observations with negative silhouette are probably placed in the wrong cluster.
# fviz_silhouette(km_res2)
# predictedCluster <- as.vector(cl_predict(km.res2,testData))
# testData$Predicted = predictedCluster
# testData$Predicted[testData$Predicted <=2] = 0
# testData$Predicted[testData$Predicted >2] = 1
# print(table(as.factor(actualData$Happy),as.factor(testData$Predicted)))
# confusionMatrix(as.factor(actualData$Happy),as.factor(testData$Predicted))
#We get accuracy of 67.65% using this method

```

```{r}
#Hierarchical clustering
##For k=2
library(dendextend)
library(colorspace)
distCalc <- dist(rawData)
#Using the "complete" method for clustering as it gives better accuracy
hclusters <- hclust(distCalc, method = "complete")
happyLevels <- rev(levels(as.factor(rawData[,17])))

#Forming the dendrogram
dend <- as.dendrogram(hclusters)
dend <- rotate(dend, 1:150)
dend <- color_branches(dend, k=2)
labels_colors(dend) <-
  rainbow_hcl(2)[sort_levels_values(
    as.numeric(rawData[,17])[order.dendrogram(dend)]
  )]

labels(dend) <- paste(as.character(rawData[,17])[order.dendrogram(dend)],"(",labels(dend),")", sep = "")
dend <- hang.dendrogram(dend,hang_height=0.1)
dend <- set(dend, "labels_cex", 0.5)

#Plotting the dendrogram
par(mfrow=c(1,1))
plot(dend, 
     main = "Clustered data set", 
     horiz =  TRUE,  nodePar = list(cex = .007))
legend("topleft", legend = happyLevels, fill = rainbow_hcl(2))

hclusteringMethod <- c("complete")

#hclusteringMethod <- c("ward.D", "single", "complete", "average", "mcquitty", "median", #"centroid", "ward.D2")
hclustDendlist <- dendlist()
for(i in seq_along(hclusteringMethod)) {
  hCluster <- hclust(distCalc , method = hclusteringMethod[i])
  hclustDendlist <- dendlist(hclustDendlist, as.dendrogram(hCluster))
}
names(hclustDendlist) <- hclust_methods
hclustDendlist

# par(mfrow = c(4,2))
# for(i in 1:8) {
#    hclustDendlist[[i]] %>% set("branches_k_color", k=2) %>% plot(axes = FALSE, horiz = TRUE)
#    title(names(hclustDendlist)[i])
# }

getClusters <- function(dend) {
  cutree(dend, k =2)[order.dendrogram(dend)]
}
dendClusters <- lapply(hclustDendlist, getClusters)
dendClusters<-as.data.frame(dendClusters)
#View(dendClusters)
colnames(dendClusters)[1]<-"predicted.clusters"
modifiedDendClusters<-dendClusters
for (i in 1:986)
{
  #If it is cluster 1,Happy value =0
  if(modifiedDendClusters$predicted.clusters[i]==1)
    modifiedDendClusters$predicted.clusters[i]<-0
  else
    #If it is cluster 2,Happy value =1
    modifiedDendClusters$predicted.clusters[i]<-1
}
#For the confusion Matrix
referenceData<-rawData$Happy
#View()
predictedData<-modifiedDendClusters$predicted.clusters
unionData <- union(predictedData,referenceData)
tableData <- table(factor(predictedData, unionData), factor(referenceData, unionData))
confusionMatrix(tableData)

```


